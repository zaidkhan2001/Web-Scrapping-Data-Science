{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3e3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "\n",
    "#Storing URLS \n",
    "Lhr_URL = \"http://lhr.nu.edu.pk/faculty/\"\n",
    "\n",
    "##### WEB SCRAPING FOR LHR WEBSITE #####\n",
    "req1 = requests.get(Lhr_URL)\n",
    "soup1 = BeautifulSoup(req1.content, 'html.parser')\n",
    "\n",
    "#Declaring Arrays for Storing Data\n",
    "Id = []\n",
    "Name = []\n",
    "Designation = []\n",
    "HEC_Status = []\n",
    "Highest_Education = []\n",
    "Email = []\n",
    "Department = []\n",
    "Extension = []\n",
    "ImageURL = []\n",
    "Profile_URL = []\n",
    "\n",
    "#Dividing With Respect With Containers\n",
    "containers = soup1.findAll('div', class_='container', id=['fsc','ee','cv','fsm','ss'])\n",
    "\n",
    "for i in range(len(containers)):\n",
    "    Cont=containers[i]\n",
    "    \n",
    "    #Finding Each Faculty Card in A Container\n",
    "    fac1 = Cont.findAll('div', class_='facultyCard')\n",
    "    \n",
    "    Dep=Cont.find('h1',class_=\"mb-2 mt-3\").text\n",
    "    \n",
    "    for j in range(len(fac1)):\n",
    "        Faculty = fac1[j]\n",
    "            \n",
    "        #Id\n",
    "        try:\n",
    "            Id.append(int(Faculty.find('a',class_='faculty-link')['href'].split('/')[-1]))\n",
    "        except:\n",
    "            Id.append(-1)\n",
    "\n",
    "        #Name\n",
    "        Name.append(Faculty.find('h5', class_='text-center').text)\n",
    "\n",
    "        #Designation\n",
    "        Temp = [item.strip() for item in Faculty.find('p', class_='small').text.strip().split('\\n') if item.strip()]\n",
    "        Designation.append(Temp[0])\n",
    "\n",
    "        #HEC Status\n",
    "        if len(Temp)>1:\n",
    "            HEC_Status.append(True)\n",
    "        else:\n",
    "            HEC_Status.append(False)\n",
    "\n",
    "        #Email\n",
    "        Email.append(Faculty.find('p', class_='mb-0').text)\n",
    "\n",
    "        #Department\n",
    "        Department.append(Dep)\n",
    "        \n",
    "        #Image URL\n",
    "        ImageURL.append('https://lhr.nu.edu.pk'+Faculty.find('img', class_='card-img-top')['src'])\n",
    "\n",
    "        #Profile URL\n",
    "        Profile_URL.append('https://lhr.nu.edu.pk'+Faculty.find('a',class_='faculty-link')['href'])\n",
    "        \n",
    "for i in range(len(Profile_URL)):\n",
    "    prof = Profile_URL[i]\n",
    "\n",
    "    req2 = requests.get(prof)\n",
    "    soup2 = BeautifulSoup(req2.content, 'html.parser')\n",
    "    \n",
    "    #Extension\n",
    "    tmp = soup2.find('span', class_='small').text.split(':')[-1].strip()\n",
    "    if tmp=='None':\n",
    "        tmp=-1\n",
    "        Extension.append(int(tmp))\n",
    "    else:\n",
    "        Extension.append(int(tmp))\n",
    "       \n",
    "    #Highest Education\n",
    "    Highest_Education.append(next((li.text.strip() for li in soup2.select('.text-justify li')), \"No Education Record\"))\n",
    "\n",
    "    \n",
    "# Create a dictionary from the arrays\n",
    "data = {\n",
    "    'Name': Name,\n",
    "    'Designation': Designation,\n",
    "    'Email': Email,\n",
    "    'Extension':Extension,\n",
    "    'ImageURL':ImageURL,\n",
    "    'ProfileURL':Profile_URL,\n",
    "    'HECstatus':HEC_Status,\n",
    "    'Highest_Education':Highest_Education,\n",
    "    'Extension':Extension,\n",
    "    'Department':Department\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df1 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb35de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "Khi_URL = \"https://khi.nu.edu.pk/\"\n",
    "\n",
    "#Declaring Arrays for Storing Data\n",
    "Id = []\n",
    "Name = []\n",
    "Designation = []\n",
    "HEC_Status = []\n",
    "Highest_Education = []\n",
    "Email = []\n",
    "Department = []\n",
    "Extension = []\n",
    "ImageURL = []\n",
    "Profile_URL = []\n",
    "\n",
    "req1 = requests.get(Khi_URL)\n",
    "soup1 = BeautifulSoup(req1.content, 'html.parser')\n",
    "\n",
    "sub = soup1.find_all('ul', class_='sub-menu')[1]\n",
    "\n",
    "List_URL = []\n",
    "\n",
    "# Traverse the items in the second submenu and extract their URLs\n",
    "for i in sub.find_all('li'):\n",
    "    a_tag = i.find('a')\n",
    "    if a_tag:\n",
    "        href = a_tag['href']\n",
    "        if href != '#':\n",
    "            List_URL.append(href)\n",
    "\n",
    "for i in range(len(List_URL)):\n",
    "    temp = List_URL[i]\n",
    "    \n",
    "    req2 = requests.get(temp)\n",
    "    soup2 = BeautifulSoup(req2.content, 'html.parser')\n",
    "    fac_Card = soup2.findAll('div', class_='gdlr-core-personnel-list-column')\n",
    "    \n",
    "    \n",
    "    for j in range(len(fac_Card)):\n",
    "        Faculty=fac_Card[j]\n",
    "    \n",
    "        #Name\n",
    "        Name.append(Faculty.find('h3', class_='gdlr-core-personnel-list-title').find('a').text.strip().split(',')[0])\n",
    "    \n",
    "        #Designation\n",
    "        Designation.append(Faculty.find('div', class_='gdlr-core-personnel-list-position').text.strip())\n",
    "        \n",
    "        #Email\n",
    "        Email.append(Faculty.find('div', class_='kingster-personnel-info-list kingster-type-email').text.strip())\n",
    "        \n",
    "        #Department\n",
    "        Department.append(soup2.find('h1', class_='kingster-page-title').text)\n",
    "        \n",
    "        #Extension\n",
    "        tmp=Faculty.find('div', class_='kingster-type-phone').get_text(strip=True)\n",
    "        if len(tmp)>1:\n",
    "            Extension.append(int(tmp))\n",
    "        else:\n",
    "            Extension.append(int(-1))\n",
    "        \n",
    "        #Profile Link\n",
    "        Profile_URL.append(Faculty.find('a')['href'])\n",
    "\n",
    "for i in range(len(Profile_URL)):\n",
    "    prof = Profile_URL[i]\n",
    "    \n",
    "    req3 = requests.get(prof)\n",
    "    soup3 = BeautifulSoup(req3.content, 'html.parser')\n",
    "    \n",
    "\n",
    "    #HEC Status\n",
    "    if 'HEC' in soup3.find('span',class_='gdlr-core-skin-caption').text.strip():\n",
    "        HEC_Status.append(\"True\")\n",
    "    else:\n",
    "        HEC_Status.append(\"False\")\n",
    "        \n",
    "    #Profile Picture\n",
    "    prof_link=soup3.find('a',class_='gdlr-core-lightgallery gdlr-core-js')\n",
    "    if prof_link:\n",
    "        ImageURL.append(prof_link['href'])\n",
    "    else:\n",
    "        ImageURL.append('NULL')\n",
    "      \n",
    "    #ID\n",
    "    id_tag = soup3.find('script', id='burst-js-extra')\n",
    "    pattern = r'\"page_id\":\"(\\d+)\"'\n",
    "    match = re.search(pattern, id_tag.string)\n",
    "    if match:\n",
    "        Id.append(int(match.group(1)))\n",
    "    else:\n",
    "        Id.append(-1)\n",
    "\n",
    "    #Education\n",
    "    booll=False\n",
    "    all_div=soup3.find_all('div',class_=\"gdlr-core-pbf-element\")\n",
    "    for x in all_div:\n",
    "        title=x.find_all('h3',class_=\"gdlr-core-title-item-title\")\n",
    "        for y in title:\n",
    "            edu=y.text\n",
    "            if edu == \"Education\":\n",
    "                booll=True\n",
    "                \n",
    "                chk=x.find('span',\"gdlr-core-title-item-caption\")\n",
    "                if chk:\n",
    "                    Highest_Education.append(chk.get_text())\n",
    "                \n",
    "                next_div = x.find_next_sibling('div', class_=\"gdlr-core-pbf-element\")\n",
    "            \n",
    "                if next_div:\n",
    "                    txt = next_div.get_text()\n",
    "                    Highest_Education.append(txt)\n",
    "                \n",
    "                else:\n",
    "                    tmp=y.find('div',\"gdlr-core-icon-list-content-wrap\")\n",
    "                    if tmp:\n",
    "                        Highest_Education.append(tmp.get_text())\n",
    "                    \n",
    "            \n",
    "    if booll == False:\n",
    "        Highest_Education.append(\"No Education\")\n",
    "        \n",
    "            \n",
    "# Create a dictionary from the arrays\n",
    "data = {\n",
    "    'Name': Name,\n",
    "    'Designation': Designation,\n",
    "    'Email': Email,\n",
    "    'Extension':Extension,\n",
    "    'ImageURL':ImageURL,\n",
    "    'ProfileURL':Profile_URL,\n",
    "    'HECstatus':HEC_Status,\n",
    "    'Highest_Education':Highest_Education,\n",
    "    'Extension':Extension,\n",
    "    'Department':Department\n",
    "}\n",
    "\n",
    "#Convert the dictionary to a DataFrame\n",
    "df2= pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9cd4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "file_path = 'ISB.html'\n",
    "\n",
    "#Declaring Arrays for Storing Data\n",
    "Id = []\n",
    "Name = []\n",
    "Designation = []\n",
    "HEC_Status = []\n",
    "Highest_Education = []\n",
    "Email = []\n",
    "Department = []\n",
    "Extension = []\n",
    "ImageURL = []\n",
    "Profile_URL = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup1 = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#Dividing HTML in Containers\n",
    "container = soup1.findAll('div', class_='container', id=True)\n",
    "\n",
    "for cont in container:\n",
    "    \n",
    "    left = cont.find('div', class_='animated fadeInLeft').text.strip()\n",
    "    right = cont.find('div', class_='animated fadeInRight').text.strip()\n",
    "    Dep = (left + \" \" + right)\n",
    "    \n",
    "    #Finding Faculty Cards in Container\n",
    "    fac_cards = cont.findAll('div', class_='col-lg-3')\n",
    "    \n",
    "    for i in range(len(fac_cards)):\n",
    "        Faculty=fac_cards[i]\n",
    "        \n",
    "        #Name\n",
    "        Name.append(Faculty.find('h3',class_='title').text.strip())\n",
    "        \n",
    "        #Designation\n",
    "        Designation.append(Faculty.find('span',class_='post').text)\n",
    "        \n",
    "        #Department\n",
    "        Department.append(Dep)\n",
    "        \n",
    "        #Email\n",
    "        Email.append(Faculty.find('p').text)\n",
    "        \n",
    "        #Profile\n",
    "        Profile_URL.append(Faculty.find('a')['href'])\n",
    "        \n",
    "        #ID\n",
    "        url = Faculty.find('a')['href']\n",
    "        Id.append(int(url[-4:]))\n",
    "\n",
    "        \n",
    "for i in range(len(Profile_URL)):\n",
    "    prof = Profile_URL[i]\n",
    "    \n",
    "    req2 = requests.get(prof)\n",
    "    soup2 = BeautifulSoup(req2.content, 'html.parser')\n",
    "    \n",
    "    #Image URL\n",
    "    img = soup2.find('div',class_='img-thumbnail')\n",
    "    ImageURL.append(\"http://isb.nu.edu.pk\" + img.find('img')['src'])\n",
    "\n",
    "    #HEC Approved\n",
    "    hec=soup2.find('label',style='font-size:9pt;')\n",
    "    if hec:\n",
    "        HEC_Status.append(True)\n",
    "    else:\n",
    "        HEC_Status.append(False)\n",
    "        \n",
    "    #Extension\n",
    "    check1=soup2.find('div',class_='profile-info')\n",
    "    check2=check1.find_all('p',style=\"line-height: 20px;margin-bottom: 0px;\")\n",
    "    check2=check2[3]\n",
    "    check3=check2.find('i',class_=\"fa fa-phone\")\n",
    "    if check3:\n",
    "        span=check2.find('span')\n",
    "        text = span.get_text(strip=True)\n",
    "        pattern = r'Ext:\\s*(\\d{3})'\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            Extension.append(int(match.group(1)))\n",
    "    else:\n",
    "        Extension.append(int(-1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c29b1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Profile_URL)):\n",
    "    prof = Profile_URL[i]\n",
    "    \n",
    "    #Education\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    dri = webdriver.Chrome(options=options)\n",
    "    dri.get(prof)\n",
    "    time.sleep(1)\n",
    "    html = dri.page_source\n",
    "    dri.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    edu = soup.find_all('span', id=\"DegreeName\")  \n",
    "    if edu:\n",
    "        temp=edu[0].find('label')\n",
    "        if temp:\n",
    "            Highest_Education.append(temp.get_text())\n",
    "        else:\n",
    "            Highest_Education.append(\"No Education Record\")\n",
    "    else:\n",
    "        Highest_Education.append(\"No Education Record\")\n",
    "\n",
    "        \n",
    "data = {\n",
    "    'Name': Name,\n",
    "    'Designation': Designation,\n",
    "    'Email': Email,\n",
    "    'Extension':Extension,\n",
    "    'ImageURL':ImageURL,\n",
    "    'ProfileURL':Profile_URL,\n",
    "    'HECstatus':HEC_Status,\n",
    "    'Highest_Education':Highest_Education,\n",
    "    'Extension':Extension,\n",
    "    'Department':Department\n",
    "}\n",
    "    \n",
    "#Convert the dictionary to a DataFrame\n",
    "df3 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae804b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 Conversion To Indvidual Faculty Files\n",
    "df1.to_csv('lhr.csv', index=False)\n",
    "df2.to_csv('khi.csv', index=False)\n",
    "df3.to_csv('isb.csv', index=False)\n",
    "\n",
    "#Step 2 Concatenation Of Files\n",
    "resultant = pd.concat([df1, df2 , df3], axis=0)\n",
    "resultant.to_csv('fast_faculty.csv',index=False)\n",
    "\n",
    "#Step 3 Creating Chunk from Concatenated File\n",
    "faculty = pd.read_csv(\"fast_faculty.csv\")\n",
    "roll_num = 2072\n",
    "last_digit = roll_num % 10\n",
    "\n",
    "if last_digit == 0:\n",
    "    val = 0.1\n",
    "else:\n",
    "    val = last_digit / 10.0\n",
    "\n",
    "_sample = faculty.sample(frac=val)\n",
    "_sample.to_csv('sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
